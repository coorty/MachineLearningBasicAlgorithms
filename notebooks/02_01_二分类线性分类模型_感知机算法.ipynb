{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机算法基本原理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**感知机**是一个最基础的二分类线性分类算法, 虽然简单, 但是可以让我们了解一个分类算法到底是怎么回事. 如下图所示, 假如我们想通过一个人的身高(X1), 体重(X2)两个特征对一个人是男生还是女生进行判别(二分类), 我们收集了3个男生的身高体重数据(正例, 如红色标记), 4个女生的身高体重数据(负例, 如紫色标记). 有了这些数据之后, 我们的目的是找到一条线(方程)将两者能恰巧分离开来, 如图中的L3,对于红色的点, 带入方程之后得到的值都大于0, 对于紫色的点带入方程后都小于0. 找到这个方程之后, 如果来了一个新的样本, 我们将其带入方程, 看是否大于0即可判断此样本是男生还是女生. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./images/02/感知机分类模型2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是如何求得这条分隔线的方程? 能使其正好将正例和反例分开? 感知机是个线性模型, 因此上图中的分隔线的方程可以假设为: $y=w_1x_1+w_2x_2+b$, 这只是输入特征维度为两维的情形, 感知机更一般的形式是:\n",
    "    \n",
    "假设特征空间的为: $\\chi \\subseteq R^n$ (这个的含义是输入样本可以是n维空间中任意的一个点), 输出空间为: $y=\\{+1, -1\\}$ (这个的含义是实例的类别), 由输入空间到输出空间的关系为:\n",
    "$$\n",
    "f(x) = sign(w^T x+b)\n",
    "$$\n",
    "\n",
    "其中, $w\\subset R^n$是模型的参数, 称为权重, 这是个列向量, 在本例中为$[w_1,w_2]'$, $b\\subset R$是模型的偏置, 是个标量. $x$是输入样本向量. **注意**: 这里面的向量默认都是列向量, 带了'的表示向量的转置(即将列向量转换称为行向量或者反之), $w^T x$是用了线性代数里面的矩阵乘法, 行向量与列向量相乘的结果是个标量. sign是个符号函数:\n",
    "$$ sign(x)=\\left\\{\n",
    "\\begin{aligned}\n",
    "+1, x \\geq 0 \\\\\n",
    "-1, x \\lt 0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "感知机模型的假设空间是定义在特征空间的所有线性分类模型, 即函数集合: $\\{f|f(x)=w^T x+b\\}$, 上面的L1~L6都在这个函数集合中. 给定了训练数据, 如何将模型参数$w, b$算出来?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机算法学习策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图中, L5比L4要差, 因为L5把一个紫色样本点分错了, 在求解模型的时候, 需要定义一个函数来衡量模型分类的好坏, 这个函数称为损失函数或者代价函数. 损失函数的一个选择是统计误分类点的数量, 比如L5为1, L4为0, 则L4更好. 但是这样的损失函数不是参数$\\vec{w}, b$的可导函数, 不方便求解. 因此损失函数的另一种选择是误分类点到分隔线的距离:\n",
    "$$\n",
    "\\frac{1}{||w||}|w^T x+b|\n",
    "$$\n",
    "\n",
    "对于误分类点($x_i, y_i$)而言, 必有:\n",
    "$$\n",
    "-y_i(w^T x+b)>0\n",
    "$$\n",
    "\n",
    "因此距离可以写成:\n",
    "$$\n",
    "-\\frac{1}{||w||}y_i(w^T x+b)\n",
    "$$\n",
    "\n",
    "假设共有**M**个误分类点, 则这些点的总距离可以表示成:\n",
    "$$\n",
    "-\\frac{1}{||w||}\\sum_{x_i\\subset M}y_i(w^T x_i+b)\n",
    "$$\n",
    "\n",
    "忽略$\\frac{1}{||w||}$ (思考下为什么可以忽略), 则感知机的代价函数为:\n",
    "$$\n",
    "L(w,b)=-\\sum_{x_i\\subset M}y_i(w^T x_i+b)\n",
    "$$\n",
    "\n",
    "显然, 这个代价函数是非负的, 如果全部的样本都正确分类, 则其为0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知器学习算法原始形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定训练数据集:\n",
    "$$\n",
    "T = \\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\}\n",
    "$$\n",
    "目标是求得参数$w, b$, 从而使$L(w,b)$的值达到最小, 即:\n",
    "$$\n",
    "\\min_{w,b}L(w,b) = -\\sum_{x_i\\subset M}y_i(w^T x_i+b)\n",
    "$$\n",
    "\n",
    "求解的过程采用梯度下降算法:\n",
    "$$\n",
    "\\bigtriangledown_wL(w,b)= - \\sum_{x_i\\subset M}y_ix_i\n",
    "$$\n",
    "$$\n",
    "\\bigtriangledown_{b}L(w,b)= - \\sum_{x_i\\subset M}y_i\n",
    "$$\n",
    "使用随机梯度下降算法, 每次随机选择一个误分类点来对$w,b$进行梯度下降:\n",
    "$$\n",
    "w \\gets w - \\alpha(-y_i x_i)\n",
    "$$\n",
    "$$\n",
    "b \\gets b - \\alpha(-y_i)\n",
    "$$\n",
    "这里的$\\alpha$为学习率, 在(0,1)之间取值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.chdir('../')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 3 4 5]\n",
      " [4 3 1 2 7 6 4]]\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "X = np.asarray([[1,4],[2,3],[3,1],[4,2],[3,7],[4,6],[5,4]]).T  #训练样本\n",
    "y = np.asarray([-1,-1,-1,-1,1,1,1])  # 训练样本的标签\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(x, y, w, bias):\n",
    "    \"\"\"\n",
    "    绘制前两个维度的决策边界\n",
    "    \"\"\"\n",
    "    w1, w2 = w[0], w[1]\n",
    "    x1 = np.arange(np.min(x[0,:]), np.max(x[1,:]), 0.1)\n",
    "    x2 = -w1 / w2 * x1 - bias / w2\n",
    "    plt.scatter(x[0,:], x[1,:], c=y, s=50)\n",
    "    plt.plot(x1, x2, 'r')\n",
    "    plt.show()\n",
    "    \n",
    "def loss(X, y, w, b):\n",
    "    \"\"\"\n",
    "    计算loss\n",
    "    \"\"\"\n",
    "    dist_vec = y * (np.dot(w, X) + b)\n",
    "    misclassify_flag = dist_vec <= 0\n",
    "    if sum(misclassify_flag) == 0:\n",
    "        return 0\n",
    "    return - sum(dist_vec[misclassify_flag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter sample 0, loss=6.0\n",
      "Iter sample 2, loss=3.300000000000001\n",
      "Iter sample 4, loss=8.000000000000002\n",
      "Iter sample 0, loss=6.6000000000000005\n",
      "Iter sample 0, loss=1.7\n",
      "Iter sample 4, loss=8.799999999999999\n",
      "Iter sample 0, loss=6.200000000000002\n",
      "Iter sample 1, loss=2.1999999999999997\n",
      "Iter sample 4, loss=9.1\n",
      "Iter sample 0, loss=5.800000000000001\n",
      "Iter sample 1, loss=2.1999999999999993\n",
      "Iter sample 4, loss=9.399999999999999\n",
      "Iter sample 0, loss=5.400000000000002\n",
      "Iter sample 2, loss=2.299999999999999\n",
      "Iter sample 4, loss=7.499999999999997\n",
      "Iter sample 0, loss=6.000000000000003\n",
      "Iter sample 0, loss=0.7000000000000015\n",
      "Iter sample 4, loss=8.299999999999997\n",
      "Iter sample 0, loss=5.600000000000004\n",
      "Iter sample 1, loss=1.099999999999998\n",
      "Iter sample 4, loss=8.599999999999996\n",
      "Iter sample 0, loss=5.200000000000004\n",
      "Iter sample 2, loss=1.199999999999997\n",
      "Iter sample 4, loss=6.699999999999996\n",
      "Iter sample 0, loss=5.800000000000004\n",
      "Iter sample 0, loss=0.6000000000000021\n",
      "Iter sample 4, loss=7.499999999999996\n",
      "Iter sample 0, loss=5.400000000000005\n",
      "Iter sample 0, loss=4.718447854656915e-15\n",
      "Iter sample 4, loss=8.299999999999994\n",
      "Iter sample 0, loss=5.000000000000005\n",
      "Iter sample 2, loss=0.9999999999999976\n",
      "Iter sample 4, loss=6.399999999999993\n",
      "Iter sample 0, loss=5.600000000000006\n",
      "Iter sample 0, loss=0.3000000000000047\n",
      "Iter sample 4, loss=7.199999999999991\n",
      "Iter sample 0, loss=5.200000000000006\n",
      "Iter sample 2, loss=0.10000000000000303\n",
      "Iter sample 4, loss=5.299999999999992\n",
      "Iter sample 0, loss=5.800000000000008\n",
      "Iter sample 0, loss=0.6000000000000055\n",
      "Iter sample 4, loss=6.099999999999991\n",
      "Iter sample 0, loss=5.4000000000000075\n",
      "Iter sample 0, loss=0.10000000000000608\n",
      "Iter sample 4, loss=6.89999999999999\n",
      "Iter sample 0, loss=5.000000000000008\n",
      "Iter sample 2, loss=0.20000000000000384\n",
      "Iter sample 4, loss=4.999999999999989\n",
      "Iter sample 0, loss=5.6000000000000085\n",
      "Iter sample 0, loss=0.40000000000000724\n",
      "Iter sample 4, loss=5.799999999999987\n",
      "Iter sample 0, loss=5.20000000000001\n",
      "Iter sample 3, loss=0.20000000000000284\n",
      "Iter sample 4, loss=6.7999999999999865\n",
      "Iter sample 0, loss=3.80000000000001\n",
      "Iter sample 6, loss=0.09999999999999576\n",
      "Iter sample 0, loss=7.8000000000000105\n",
      "Iter sample 0, loss=2.4000000000000106\n",
      "Iter sample 4, loss=2.799999999999985\n",
      "Iter sample 0, loss=7.400000000000011\n",
      "Iter sample 1, loss=2.100000000000009\n",
      "Iter sample 4, loss=3.0999999999999845\n",
      "Iter sample 0, loss=7.000000000000011\n",
      "Iter sample 1, loss=1.8000000000000087\n",
      "Iter sample 4, loss=3.3999999999999853\n",
      "Iter sample 0, loss=6.600000000000011\n",
      "Iter sample 1, loss=1.5000000000000084\n",
      "Iter sample 4, loss=3.699999999999985\n",
      "Iter sample 0, loss=6.200000000000011\n",
      "Iter sample 1, loss=1.2000000000000082\n",
      "Iter sample 4, loss=3.999999999999986\n",
      "Iter sample 0, loss=5.8000000000000105\n",
      "Iter sample 2, loss=1.0000000000000053\n",
      "Iter sample 4, loss=2.0999999999999854\n",
      "Iter sample 0, loss=6.400000000000012\n",
      "Iter sample 0, loss=1.0000000000000109\n",
      "Iter sample 4, loss=2.8999999999999835\n",
      "Iter sample 0, loss=6.000000000000012\n",
      "Iter sample 1, loss=0.9000000000000088\n",
      "Iter sample 4, loss=3.1999999999999837\n",
      "Iter sample 0, loss=5.600000000000013\n",
      "Iter sample 2, loss=0.7000000000000064\n",
      "Iter sample 4, loss=1.2999999999999823\n",
      "Iter sample 0, loss=6.2000000000000135\n",
      "Iter sample 0, loss=1.0000000000000104\n",
      "Iter sample 4, loss=2.099999999999981\n",
      "Iter sample 0, loss=5.800000000000014\n",
      "Iter sample 1, loss=0.7000000000000077\n",
      "Iter sample 4, loss=2.3999999999999795\n",
      "Iter sample 0, loss=5.4000000000000155\n",
      "Iter sample 3, loss=0.600000000000005\n",
      "Iter sample 4, loss=3.399999999999978\n",
      "Iter sample 0, loss=4.000000000000016\n",
      "Get final w = [0.3 0.4]\n",
      "Get final b = -2.1000000000000005\n"
     ]
    }
   ],
   "source": [
    "# 迭代求解, 每次只取一个误分类样本\n",
    "# 参数初始化\n",
    "alpha = 0.1  # 学习率\n",
    "w = np.asarray([0.1,0.1])  # 初始的参数w\n",
    "b = 1  # 初始的b\n",
    "n = len(y)  # 训练样本的数量\n",
    "i = 0\n",
    "while True:\n",
    "    if y[i] * (np.dot(w, X[:, i]) + b) <= 0:\n",
    "        print(\"Iter sample {}, loss={}\".format(i, loss(X, y, w, b)))\n",
    "        w = w + alpha * y[i] * X[:,i]\n",
    "        b = b + alpha * y[i]\n",
    "        i = 0  # 重置i\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "    if i == n:\n",
    "        print(\"Get final w = {}\".format(w))\n",
    "        print(\"Get final b = {}\".format(b))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decision_boundary(X, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面每次只取一个样本去更新梯度, 这种称为随机梯度下降, 当然, 更普遍的做法是每次使用所有的误分类样本来做梯度下降!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: misclassify num: 1, loss: -0.0\n",
      "Iter 1: misclassify num: 3, loss: 38.1\n",
      "Iter 2: misclassify num: 4, loss: 118.0\n",
      "Iter 3: misclassify num: 3, loss: 12.200000000000003\n",
      "Iter 4: misclassify num: 3, loss: 91.1\n",
      "Iter 5: misclassify num: 4, loss: 75.0\n",
      "Iter 6: misclassify num: 3, loss: 21.099999999999994\n",
      "Iter 7: misclassify num: 4, loss: 118.0\n",
      "Iter 8: misclassify num: 2, loss: 20.200000000000003\n",
      "Iter 9: misclassify num: 3, loss: 31.599999999999994\n",
      "Iter 10: misclassify num: 4, loss: 107.0\n",
      "Iter 11: misclassify num: 2, loss: 15.700000000000005\n",
      "Iter 12: misclassify num: 3, loss: 42.099999999999994\n",
      "Iter 13: misclassify num: 4, loss: 96.0\n",
      "Iter 14: misclassify num: 3, loss: 14.100000000000003\n",
      "Iter 15: misclassify num: 2, loss: 8.8\n",
      "Iter 16: misclassify num: 2, loss: 8.700000000000003\n",
      "Iter 17: misclassify num: 3, loss: 43.599999999999994\n",
      "Iter 18: misclassify num: 4, loss: 98.0\n",
      "Iter 19: misclassify num: 1, loss: 4.400000000000002\n",
      "Iter 20: misclassify num: 3, loss: 15.099999999999994\n",
      "Iter 21: misclassify num: 4, loss: 114.0\n",
      "Iter 22: misclassify num: 2, loss: 14.200000000000005\n",
      "Iter 23: misclassify num: 3, loss: 25.599999999999994\n",
      "Iter 24: misclassify num: 4, loss: 103.0\n",
      "Iter 25: misclassify num: 2, loss: 9.700000000000005\n",
      "Iter 26: misclassify num: 3, loss: 36.099999999999994\n",
      "Iter 27: misclassify num: 4, loss: 92.0\n",
      "Iter 28: misclassify num: 1, loss: 5.900000000000002\n",
      "Iter 29: misclassify num: 2, loss: 11.299999999999997\n",
      "Iter 30: misclassify num: 4, loss: 56.0\n",
      "Iter 31: misclassify num: 2, loss: 17.799999999999997\n",
      "Iter 32: misclassify num: 4, loss: 47.0\n",
      "Iter 33: misclassify num: 3, loss: 25.599999999999994\n",
      "Iter 34: misclassify num: 4, loss: 90.0\n",
      "Iter 35: misclassify num: 1, loss: 8.400000000000002\n",
      "Iter 36: misclassify num: 1, loss: 9.399999999999999\n",
      "Iter 37: misclassify num: 2, loss: 14.200000000000006\n",
      "Iter 38: misclassify num: 2, loss: 13.799999999999997\n",
      "Iter 39: misclassify num: 4, loss: 47.0\n",
      "Iter 40: misclassify num: 3, loss: 21.099999999999994\n",
      "Iter 41: misclassify num: 4, loss: 90.0\n",
      "Iter 42: misclassify num: 1, loss: 6.900000000000002\n",
      "Iter 43: misclassify num: 1, loss: 6.899999999999999\n",
      "Iter 44: misclassify num: 2, loss: 12.200000000000006\n",
      "Iter 45: misclassify num: 2, loss: 9.799999999999997\n",
      "Iter 46: misclassify num: 4, loss: 47.0\n",
      "Iter 47: misclassify num: 3, loss: 16.599999999999994\n",
      "Iter 48: misclassify num: 4, loss: 90.0\n",
      "Iter 49: misclassify num: 1, loss: 5.400000000000002\n",
      "Iter 50: misclassify num: 1, loss: 4.399999999999999\n",
      "Iter 51: misclassify num: 3, loss: 10.700000000000006\n",
      "Iter 52: misclassify num: 3, loss: 45.599999999999994\n",
      "Iter 53: misclassify num: 4, loss: 67.0\n",
      "Iter 54: misclassify num: 1, loss: 1.8999999999999986\n",
      "Iter 55: misclassify num: 3, loss: 15.200000000000003\n",
      "Iter 56: misclassify num: 3, loss: 33.099999999999994\n",
      "Iter 57: misclassify num: 4, loss: 71.0\n",
      "Iter 58: misclassify num: 1, loss: 1.4000000000000021\n",
      "Iter 59: misclassify num: 1, loss: 10.399999999999999\n",
      "Iter 60: misclassify num: 1, loss: 3.400000000000002\n",
      "Iter 61: misclassify num: 1, loss: 0.3999999999999986\n",
      "Iter 62: misclassify num: 3, loss: 10.700000000000006\n",
      "Iter 63: misclassify num: 3, loss: 38.099999999999994\n",
      "Iter 64: misclassify num: 4, loss: 68.0\n",
      "Iter 65: misclassify num: 0, loss: 0\n",
      "Get final w = [1.3 5.9]\n",
      "Get final b = -28.0\n"
     ]
    }
   ],
   "source": [
    "# 迭代求解2: 每次使用所有的误分类样本\n",
    "alpha = 0.5  # 学习率\n",
    "w2 = [0.3, 0.4]\n",
    "b2 = -2\n",
    "dist_vec = y * (np.dot(w2, X) + b2)\n",
    "misclassify_flag = dist_vec <= 0\n",
    "index = 0\n",
    "print(\"Iter {}: misclassify num: {}, loss: {}\".format(index, np.sum(misclassify_flag), loss(X,y,w2,b2)))\n",
    "while np.sum(misclassify_flag) > 0:\n",
    "    index += 1\n",
    "    w2 = w2 + alpha * np.sum(y[misclassify_flag] * X[:, misclassify_flag], axis=1)\n",
    "    b2  =  b2 + alpha * np.sum(y[misclassify_flag])\n",
    "    dist_vec = y * (np.dot(w2, X) + b2)\n",
    "    misclassify_flag = dist_vec <= 0\n",
    "    print(\"Iter {}: misclassify num: {}, loss: {}\".format(index, np.sum(misclassify_flag), loss(X,y,w2,b2)))\n",
    "    \n",
    "print(\"Get final w = {}\".format(w2))\n",
    "print(\"Get final b = {}\".format(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decision_boundary(X, y, w2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思考1**: 每次使用所有误分类样本来更新梯度, loss为何忽大忽小? 因为一般而言模型的loss都是使用所有样本计算出来的(不管是正确分类还是错误分类), 而感知机比较特殊, 它只使用误分类样本来计算误差, 利用误分类样本更新完梯度后, 又可能造成了原本正确分类的样本分错了, 因此这个loss不稳定."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知器学习算法对偶形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面讲到: 每次随机选择一个误分类点来对$\\boldsymbol{w},b$进行梯度下降:\n",
    "$$\n",
    "\\boldsymbol{w} \\gets \\boldsymbol{w} + \\eta y_i\\boldsymbol{x_i}\n",
    "$$\n",
    "$$\n",
    "b \\gets b + \\eta y_i\n",
    "$$\n",
    "\n",
    "初始的$\\boldsymbol{w}, b$可以都赋值为0, 则最终得到的$\\boldsymbol{w}, b$可以表示为实例X和标签y的线性函数:\n",
    "$$\n",
    "\\boldsymbol{w} = \\sum_{i=1}^N \\eta_i y_i \\boldsymbol{x_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\sum_{i=1}^N \\eta_i y_i\n",
    "$$\n",
    "\n",
    "这里$\\eta_i \\ge 0, i=1,...,N$. 则感知机模型为:\n",
    "$$\n",
    "f(x) = sign(\\sum_{j=1}^N \\eta_j y_j x^T_j x + \\eta_j y_j)\n",
    "$$\n",
    "\n",
    "代价函数为:\n",
    "\n",
    "$$\n",
    "L(w,b) = L(\\eta)= - \\sum_{x_i \\subset M} y_i (\\sum_{j=1}^N \\eta_j y_j x^T_j x_i + \\eta_j y_j) \\\n",
    "= -\\sum_{x_i \\subset M} y_i (\\sum_{j=1}^N y_j \\eta_j (x^T_j x_i + 1))\n",
    "$$\n",
    "\n",
    "对$\\eta_t$求偏导:\n",
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bigtriangledown_{\\eta_t}L(\\eta) = - \\sum_{x_i \\subset M} y_i y_t (x^T_t x_i + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则梯度更新公式为 (当样本t被错分的时候才更新$\\eta_t$): \n",
    "$$\n",
    "\\eta_t = \\eta_t + \\alpha \\sum_{x_i \\subset M} y_i y_t (x^T_t x_i + 1)\n",
    "$$\n",
    "\n",
    "如果每次只用一个错分点进行更新:\n",
    "$$\n",
    "\\eta_t = \\eta_t + \\alpha  y_t y_t (x^T_t x_t + 1) = \\eta_t + \\alpha (G(t,t) + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_v2(X, y, eta):\n",
    "    \"\"\"\n",
    "    计算loss\n",
    "    \"\"\"\n",
    "    dist_vec = np.dot(eta * y, 1 + G) * y \n",
    "    misclassify_flag = dist_vec <= 0\n",
    "    if sum(misclassify_flag) == 0:\n",
    "        return 0\n",
    "    return -sum(dist_vec[misclassify_flag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: misclassify num: 7, dist_vec:[-0. -0. -0. -0.  0.  0.  0.], loss: -0.0\n",
      "Iter 1: misclassify num: 4, dist_vec: [-521.4 -489.2 -350.8 -531.   988.   955.8  817.4], loss: 1892.4\n",
      "Iter 2: misclassify num: 4, dist_vec: [-224.8 -192.6 -109.2 -179.4  416.4  384.2  300.8], loss: 706.0000000000002\n",
      "Iter 3: misclassify num: 3, dist_vec: [  71.8  104.   132.4  172.2 -155.2 -187.4 -215.8], loss: 558.4000000000001\n",
      "Iter 4: misclassify num: 4, dist_vec: [-1165.7 -1052.8  -688.9 -1078.   2194.9  2082.   1718.1], loss: 3985.4000000000005\n",
      "Iter 5: misclassify num: 4, dist_vec: [-869.1 -756.2 -447.3 -726.4 1623.3 1510.4 1201.5], loss: 2799.0000000000005\n",
      "Iter 6: misclassify num: 4, dist_vec: [-572.5 -459.6 -205.7 -374.8 1051.7  938.8  684.9], loss: 1612.6000000000004\n",
      "Iter 7: misclassify num: 3, dist_vec: [-275.9 -163.    35.9  -23.2  480.1  367.2  168.3], loss: 462.1000000000009\n",
      "Iter 8: misclassify num: 3, dist_vec: [ -63.4   41.1  190.2  205.5   77.4  -27.1 -176.2], loss: 266.69999999999914\n",
      "Iter 9: misclassify num: 4, dist_vec: [-465.9 -344.2  -97.3 -226.   848.5  726.8  479.9], loss: 1133.4000000000005\n",
      "Iter 10: misclassify num: 3, dist_vec: [-169.3  -47.6  144.3  125.6  276.9  155.2  -36.7], loss: 253.6000000000007\n",
      "Iter 11: misclassify num: 3, dist_vec: [-133.9  -15.8  165.1  157.6  211.7   93.6  -87.3], loss: 237.00000000000068\n",
      "Iter 12: misclassify num: 2, dist_vec: [ -98.5   16.   185.9  189.6  146.5   32.  -137.9], loss: 236.40000000000055\n",
      "Iter 13: misclassify num: 3, dist_vec: [-149.7  -36.   142.7  126.4  247.3  133.6  -45.1], loss: 230.8000000000004\n",
      "Iter 14: misclassify num: 3, dist_vec: [-114.3   -4.2  163.5  158.4  182.1   72.   -95.7], loss: 214.20000000000016\n",
      "Iter 15: misclassify num: 2, dist_vec: [ -78.9   27.6  184.3  190.4  116.9   10.4 -146.3], loss: 225.20000000000005\n",
      "Iter 16: misclassify num: 3, dist_vec: [-130.1  -24.4  141.1  127.2  217.7  112.   -53.5], loss: 207.9999999999999\n",
      "Iter 17: misclassify num: 2, dist_vec: [ -94.7    7.4  161.9  159.2  152.5   50.4 -104.1], loss: 198.79999999999973\n",
      "Iter 18: misclassify num: 3, dist_vec: [-145.9  -44.6  118.7   96.   253.3  152.   -11.3], loss: 201.8000000000003\n",
      "Iter 19: misclassify num: 3, dist_vec: [-110.5  -12.8  139.5  128.   188.1   90.4  -61.9], loss: 185.20000000000027\n",
      "Iter 20: misclassify num: 2, dist_vec: [ -75.1   19.   160.3  160.   122.9   28.8 -112.5], loss: 187.60000000000036\n",
      "Iter 21: misclassify num: 3, dist_vec: [-126.3  -33.   117.1   96.8  223.7  130.4  -19.7], loss: 178.99999999999977\n",
      "Iter 22: misclassify num: 3, dist_vec: [-90.9  -1.2 137.9 128.8 158.5  68.8 -70.3], loss: 162.39999999999986\n",
      "Iter 23: misclassify num: 2, dist_vec: [ -55.5   30.6  158.7  160.8   93.3    7.2 -120.9], loss: 176.4000000000001\n",
      "Iter 24: misclassify num: 3, dist_vec: [-106.7  -21.4  115.5   97.6  194.1  108.8  -28.1], loss: 156.19999999999948\n",
      "Iter 25: misclassify num: 2, dist_vec: [-71.3  10.4 136.3 129.6 128.9  47.2 -78.7], loss: 150.00000000000023\n",
      "Iter 26: misclassify num: 2, dist_vec: [-122.5  -41.6   93.1   66.4  229.7  148.8   14.1], loss: 164.09999999999934\n",
      "Iter 27: misclassify num: 3, dist_vec: [ -19.6   48.5  148.5  152.8   42.9  -25.2 -125.2], loss: 170.00000000000068\n",
      "Iter 28: misclassify num: 4, dist_vec: [-422.1 -336.8 -139.  -278.7  814.   728.7  530.9], loss: 1176.599999999998\n",
      "Iter 29: misclassify num: 2, dist_vec: [-125.5  -40.2  102.6   72.9  242.4  157.1   14.3], loss: 165.69999999999902\n",
      "Iter 30: misclassify num: 3, dist_vec: [ -22.6   49.9  158.   159.3   55.6  -16.9 -125. ], loss: 164.50000000000068\n",
      "Iter 31: misclassify num: 4, dist_vec: [-425.1 -335.4 -129.5 -272.2  826.7  737.   531.1], loss: 1162.1999999999985\n",
      "Iter 32: misclassify num: 2, dist_vec: [-128.5  -38.8  112.1   79.4  255.1  165.4   14.5], loss: 167.29999999999882\n",
      "Iter 33: misclassify num: 3, dist_vec: [ -25.6   51.3  167.5  165.8   68.3   -8.6 -124.8], loss: 159.0000000000016\n",
      "Iter 34: misclassify num: 4, dist_vec: [-428.1 -334.  -120.  -265.7  839.4  745.3  531.3], loss: 1147.7999999999988\n",
      "Iter 35: misclassify num: 2, dist_vec: [-131.5  -37.4  121.6   85.9  267.8  173.7   14.7], loss: 168.89999999999895\n",
      "Iter 36: misclassify num: 3, dist_vec: [ -28.6   52.7  177.   172.3   81.    -0.3 -124.6], loss: 153.49999999999955\n",
      "Iter 37: misclassify num: 4, dist_vec: [-431.1 -332.6 -110.5 -259.2  852.1  753.6  531.5], loss: 1133.3999999999976\n",
      "Iter 38: misclassify num: 2, dist_vec: [-134.5  -36.   131.1   92.4  280.5  182.    14.9], loss: 170.4999999999984\n",
      "Iter 39: misclassify num: 2, dist_vec: [ -31.6   54.1  186.5  178.8   93.7    8.  -124.4], loss: 155.99999999999977\n",
      "Iter 40: misclassify num: 2, dist_vec: [-82.8   2.1 143.3 115.6 194.5 109.6 -31.6], loss: 114.39999999999986\n",
      "Iter 41: misclassify num: 2, dist_vec: [-134.   -49.9  100.1   52.4  295.3  211.2   61.2], loss: 183.89999999999804\n",
      "Iter 42: misclassify num: 2, dist_vec: [-31.1  40.2 155.5 138.8 108.5  37.2 -78.1], loss: 109.19999999999959\n",
      "Iter 43: misclassify num: 2, dist_vec: [-82.3 -11.8 112.3  75.6 209.3 138.8  14.7], loss: 94.09999999999786\n",
      "Iter 44: misclassify num: 2, dist_vec: [  20.6   78.3  167.7  162.    22.5  -35.2 -124.6], loss: 159.800000000002\n",
      "Iter 45: misclassify num: 4, dist_vec: [-455.  -386.4 -192.5 -374.5  944.3  875.7  681.8], loss: 1408.3999999999974\n",
      "Iter 46: misclassify num: 3, dist_vec: [-158.4  -89.8   49.1  -22.9  372.7  304.1  165.2], loss: 271.09999999999764\n",
      "Iter 47: misclassify num: 3, dist_vec: [  54.1  114.3  203.4  205.8  -30.   -90.2 -179.3], loss: 299.50000000000546\n",
      "Iter 48: misclassify num: 4, dist_vec: [-1183.4 -1042.5  -617.9 -1044.4  2320.1  2179.2  1754.6], loss: 3888.1999999999953\n",
      "Iter 49: misclassify num: 4, dist_vec: [-886.8 -745.9 -376.3 -692.8 1748.5 1607.6 1238. ], loss: 2701.7999999999943\n",
      "Iter 50: misclassify num: 4, dist_vec: [-590.2 -449.3 -134.7 -341.2 1176.9 1036.   721.4], loss: 1515.3999999999937\n",
      "Iter 51: misclassify num: 2, dist_vec: [-293.6 -152.7  106.9   10.4  605.3  464.4  204.8], loss: 446.29999999999495\n",
      "Iter 52: misclassify num: 2, dist_vec: [-190.7  -62.6  162.3   96.8  418.5  290.4   65.5], loss: 253.29999999999586\n",
      "Iter 53: misclassify num: 2, dist_vec: [-87.8  27.5 217.7 183.2 231.7 116.4 -73.8], loss: 161.60000000000127\n",
      "Iter 54: misclassify num: 2, dist_vec: [-139.   -24.5  174.5  120.   332.5  218.    19. ], loss: 163.4999999999966\n",
      "Iter 55: misclassify num: 2, dist_vec: [ -36.1   65.6  229.9  206.4  145.7   44.  -120.3], loss: 156.40000000000146\n",
      "Iter 56: misclassify num: 2, dist_vec: [-87.3  13.6 186.7 143.2 246.5 145.6 -27.5], loss: 114.80000000000018\n",
      "Iter 57: misclassify num: 2, dist_vec: [-138.5  -38.4  143.5   80.   347.3  247.2   65.3], loss: 176.8999999999951\n",
      "Iter 58: misclassify num: 2, dist_vec: [-35.6  51.7 198.9 166.4 160.5  73.2 -74. ], loss: 109.60000000000036\n",
      "Iter 59: misclassify num: 2, dist_vec: [-86.8  -0.3 155.7 103.2 261.3 174.8  18.8], loss: 87.0999999999949\n",
      "Iter 60: misclassify num: 1, dist_vec: [  16.1   89.8  211.1  189.6   74.5    0.8 -120.5], loss: 120.50000000000273\n",
      "Iter 61: misclassify num: 2, dist_vec: [-76.3  -6.8 127.1  67.8 259.3 189.8  55.9], loss: 83.09999999999536\n",
      "Iter 62: misclassify num: 1, dist_vec: [ 26.6  83.3 182.5 154.2  72.5  15.8 -83.4], loss: 83.40000000000418\n",
      "Iter 63: misclassify num: 2, dist_vec: [-65.8 -13.3  98.5  32.4 257.3 204.8  93. ], loss: 79.09999999999627\n",
      "Iter 64: misclassify num: 1, dist_vec: [ 37.1  76.8 153.9 118.8  70.5  30.8 -46.3], loss: 46.30000000000291\n",
      "Iter 65: misclassify num: 3, dist_vec: [-55.3 -19.8  69.9  -3.  255.3 219.8 130.1], loss: 78.09999999999445\n",
      "Iter 66: misclassify num: 3, dist_vec: [ 157.2  184.3  224.2  225.7 -147.4 -174.5 -214.4], loss: 536.3000000000102\n",
      "Iter 67: misclassify num: 4, dist_vec: [-1080.3  -972.5  -597.1 -1024.5  2202.7  2094.9  1719.5], loss: 3674.399999999991\n",
      "Iter 68: misclassify num: 4, dist_vec: [-783.7 -675.9 -355.5 -672.9 1631.1 1523.3 1202.9], loss: 2487.999999999992\n",
      "Iter 69: misclassify num: 4, dist_vec: [-487.1 -379.3 -113.9 -321.3 1059.5  951.7  686.3], loss: 1301.5999999999904\n",
      "Iter 70: misclassify num: 2, dist_vec: [-190.5  -82.7  127.7   30.3  487.9  380.1  169.7], loss: 273.199999999993\n",
      "Iter 71: misclassify num: 1, dist_vec: [-87.6   7.4 183.1 116.7 301.1 206.1  30.4], loss: 87.59999999999673\n",
      "Iter 72: misclassify num: 2, dist_vec: [-55.2  34.4 197.5 140.1 243.5 153.9  -9.2], loss: 64.39999999999964\n",
      "Iter 73: misclassify num: 2, dist_vec: [-106.4  -17.6  154.3   76.9  344.3  255.5   83.6], loss: 123.99999999999318\n",
      "Iter 74: misclassify num: 2, dist_vec: [ -3.5  72.5 209.7 163.3 157.5  81.5 -55.7], loss: 59.19999999999891\n",
      "Iter 75: misclassify num: 1, dist_vec: [-54.7  20.5 166.5 100.1 258.3 183.1  37.1], loss: 54.69999999999709\n",
      "Iter 76: misclassify num: 2, dist_vec: [-22.3  47.5 180.9 123.5 200.7 130.9  -2.5], loss: 24.80000000000109\n",
      "Iter 77: misclassify num: 2, dist_vec: [-73.5  -4.5 137.7  60.3 301.5 232.5  90.3], loss: 77.99999999999318\n",
      "Iter 78: misclassify num: 1, dist_vec: [ 29.4  85.6 193.1 146.7 114.7  58.5 -49. ], loss: 49.00000000000546\n",
      "Iter 79: misclassify num: 2, dist_vec: [-63.  -11.  109.1  24.9 299.5 247.5 127.4], loss: 73.99999999999272\n",
      "Iter 80: misclassify num: 1, dist_vec: [ 39.9  79.1 164.5 111.3 112.7  73.5 -11.9], loss: 11.900000000005093\n",
      "Iter 81: misclassify num: 3, dist_vec: [-52.5 -17.5  80.5 -10.5 297.5 262.5 164.5], loss: 80.49999999998545\n",
      "Iter 82: misclassify num: 3, dist_vec: [ 160.   186.6  234.8  218.2 -105.2 -131.8 -180. ], loss: 417.0000000000209\n",
      "Iter 83: misclassify num: 4, dist_vec: [-1077.5  -970.2  -586.5 -1032.   2244.9  2137.6  1753.9], loss: 3666.199999999984\n",
      "Iter 84: misclassify num: 4, dist_vec: [-780.9 -673.6 -344.9 -680.4 1673.3 1566.  1237.3], loss: 2479.7999999999834\n",
      "Iter 85: misclassify num: 4, dist_vec: [-484.3 -377.  -103.3 -328.8 1101.7  994.4  720.7], loss: 1293.399999999981\n",
      "Iter 86: misclassify num: 2, dist_vec: [-187.7  -80.4  138.3   22.8  530.1  422.8  204.1], loss: 268.09999999998763\n",
      "Iter 87: misclassify num: 1, dist_vec: [-84.8   9.7 193.7 109.2 343.3 248.8  64.8], loss: 84.79999999999518\n",
      "Iter 88: misclassify num: 1, dist_vec: [-52.4  36.7 208.1 132.6 285.7 196.6  25.2], loss: 52.39999999999554\n",
      "Iter 89: misclassify num: 2, dist_vec: [-20.   63.7 222.5 156.  228.1 144.4 -14.4], loss: 34.40000000000373\n",
      "Iter 90: misclassify num: 1, dist_vec: [-71.2  11.7 179.3  92.8 328.9 246.   78.4], loss: 71.19999999999482\n",
      "Iter 91: misclassify num: 1, dist_vec: [-38.8  38.7 193.7 116.2 271.3 193.8  38.8], loss: 38.79999999999518\n",
      "Iter 92: misclassify num: 2, dist_vec: [ -6.4  65.7 208.1 139.6 213.7 141.6  -0.8], loss: 7.20000000000482\n",
      "Iter 93: misclassify num: 1, dist_vec: [-57.6  13.7 164.9  76.4 314.5 243.2  92. ], loss: 57.59999999999445\n",
      "Iter 94: misclassify num: 1, dist_vec: [-25.2  40.7 179.3  99.8 256.9 191.   52.4], loss: 25.199999999994816\n",
      "Iter 95: misclassify num: 0, dist_vec: [  7.2  67.7 193.7 123.2 199.3 138.8  12.8], loss: 0\n",
      "Get final eta = [187.9 184.3  86.8 145.5  68.6 110.4 151.3]\n",
      "Get final w = [ 5.  65.5]\n",
      "Get final b = -274.2000000000004\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "X = np.asarray([[1,4],[2,3],[3,1],[4,2],[3,7],[4,6],[5,4]]).T  #训练样本\n",
    "y = np.asarray([-1,-1,-1,-1,1,1,1])  # 训练样本的标签\n",
    "alpha = 0.1  # 学习率\n",
    "eta = np.array([0.0] * n)  # 待求参数\n",
    "G = np.dot(X.T, X)  # Gram矩阵\n",
    "dist_vec = np.dot(eta * y, 1 + G) * y \n",
    "misclassify_flag = dist_vec <= 0\n",
    "index = 0\n",
    "print(\"Iter {}: misclassify num: {}, dist_vec:{}, loss: {}\".format(index, np.sum(misclassify_flag), dist_vec, loss_v2(X,y,eta)))\n",
    "while sum(misclassify_flag) > 0:\n",
    "    index += 1\n",
    "    for i in range(n):\n",
    "        if misclassify_flag[i]:\n",
    "            eta[i] += alpha * np.dot(y[misclassify_flag]* y[i], G[i, misclassify_flag] + 1)\n",
    "    dist_vec = np.dot(eta * y, 1 + G) * y \n",
    "    misclassify_flag = dist_vec <= 0\n",
    "    print(\"Iter {}: misclassify num: {}, dist_vec: {}, loss: {}\".format(index, np.sum(misclassify_flag), dist_vec, loss_v2(X,y,eta)))\n",
    "    \n",
    "w3 = np.dot(X, eta * y)\n",
    "b3 = np.sum(eta * y) \n",
    "print(\"Get final eta = {}\".format(eta))\n",
    "print(\"Get final w = {}\".format(w3))\n",
    "print(\"Get final b = {}\".format(b3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decision_boundary(X, y, w3, b3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
